
train:
  model:
    name: "Bllossom/llama-3.2-Korean-Bllossom-3B"
    #name : "NCSOFT/Llama-VARCO-8B-Instruct" 
    quantization:
      use: True
      config:
        load_in_8bit: True
        #load_in_4bit: True
        #bnb_4bit_use_double_quant: True
        #bnb_4bit_quant_type: "nf4"
        bnb_4bit_compute_dtype: bfloat16

    lora:
      use: True
      config:
        task_type: "CAUSAL_LM"
        r: 64 
        lora_alpha: 16
        lora_dropout: 0.1
        use_dora: True
      #  target_modules: ["q_proj", "v_proj", "k_proj"]  # 실제 모델의 모듈 이름에 맞게 설정

  parameter_save: "/home/aicontest/construct/experiment/src/parameter"

  SFT: True
  training:
    output_dir: "./outputs"
    per_device_train_batch_size: 4
    # max_steps: 5000
    # gradient_accumulation_steps: 4
    num_train_epochs: 1 
    save_total_limit: 1
    logging_dir: "./logs"
    logging_steps: 10
    learning_rate: 0.0002
    weight_decay: 0.1
    # fp16: True  # mixed precision 사용
    push_to_hub: False
    report_to: "none"
    
  dpo:
    output_dir: "./dpo_model"
    gradient_accumulation_steps: 4   # 배치 사이즈가 작은 대신 그라디언트 누적
    # fp16 : True         # 반정밀도 사용
    max_steps: 4000                 
    gradient_checkpointing: True     
    per_device_train_batch_size: 1
    num_train_epochs: 1
    logging_steps: 10
    save_total_limit: 1
    weight_decay: 0.1
    push_to_hub: False
    report_to: "none"
  
  resume_checkpoint:  "/root/construct/experiment/outputs/checkpoint-52701"
  resume : False

  test:
    dir: "Trainer_3B_Dora" 